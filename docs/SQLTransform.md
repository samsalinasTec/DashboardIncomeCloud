These modules use files previously downloaded by the modules in [SQL Download Main module](./SQLDownloadMain.py). Specifically, each module takes a set of CSV files, performs cleaning and necessary transformations, and outputs a single file that becomes part of our data model. In other words, each module ultimately produces a single file containing the precise information required, structured appropriately to interact seamlessly with the other tables.

In this case, we've chosen to encapsulate all modules within a single script to facilitate a smoother file creation workflow. In the complete version of the script (remembering these are only simplified examples derived from the full solution), many CSV files generated by one module are reused directly in another to reduce unnecessary steps. Although we recognize that separate pipelines could be implemented using other methods to modularize each task independently, we've opted for this approach for now.

Each module performs a specific data transformation to produce the desired output and is executed via the main function within the script [Transform Main script](./TablesTransformMain.py). The processed information is then saved again as a new CSV file. Although these could be uploaded directly to our cloud repository, we chose to store them as CSV files to ensure workflow reliability. This way, we have backups readily available if the script fails at any stage or if the virtual machine runs out of RAM during processing. Having these CSV backups simplifies the recovery and upload process, avoiding the need to re-run all data cleaning and transformations.

