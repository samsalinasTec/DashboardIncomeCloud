# Project Overview
This project was developed for a marketing company; however, for security reasons, all sensitive information has been replaced, simulating a cloud services company. Additionally, for each section of the “ETL,” only examples based on the original code have been used (since it contained explicit references to important company information). Additionally, the dashboard mentioned here is not included in this GitHub repository due to its file size. However, you can view it directly at the following URL: []

However, the code is essentially the same as what was used for the real ETL. If you want a deeper understanding of the project—whether to resolve very specific questions or to learn how certain problems were solved in order to replicate this project—feel free to contact me.

## Technologies and Tools Used:

- Python for data extraction, cleaning, and transformation
- SQL (various queries for data retrieval)
- Cron jobs hosted on a Virtual Machine (VM) for scheduling and automation
- BigQuery as the cloud-based repository for scalable data storage
- Power BI for data visualization, dashboard creation, and KPI tracking
- A star schema data model to ensure efficient queries and clear data relationships

# Project Scope
In general, this project represents a comprehensive application of knowledge in:

- Databases
- Data analysis
- Programming
- Data structures
- Data modeling and ETL workflows
- Data visualization
- KPI-based business decision-making

The core purpose is to provide essential business information to support day-to-day decision-making. To achieve this effectively, the best approach is to visualize key indicators in a dashboard that highlights relevant business aspects. This dashboard must retrieve updated data daily, which in turn requires that all underlying data sources be updated on a daily basis.

# General Workflow Overview

At first glance, the problem seems straightforward: we simply need to retrieve data from our databases and create visualizations and measures in the dashboard to track performance indicators. However, the data required to generate meaningful insights is not directly available without first applying transformations. Furthermore, a significant portion of our database is outdated, resulting in inefficient data modeling. Therefore, aside from transforming the data, we must establish a robust data model/structure that clearly represents the interactions between different data sources and is scalable for future additions. With this in mind, a star schema data model was developed. Thus, data extraction and transformations become even more critical, as they enable the creation of this structured model, ultimately providing the information necessary for informed business decision-making

# Data Extraction
The process begins with the [SQL Download Main module](./SQLDownloadMain.py) scripts. These scripts:

- Load the credentials necessary to access the databases.
- Execute queries to extract the required information.
- Download the resulting data.
- Save it into CSV files for further processing.

# Data Transformation
Once the data has been downloaded, the [Transform Main script](./TablesTransformMain.py) scripts take over.They:

- Clean the data (removing nulls, fixing data types, etc.).
- Create new columns and perform the transformations necessary to meet both technical requirements of the star schema and the specific business indicators we aim to measure.
- Produce the final tables needed by our data model. 

In this way, the raw files generated by the [SQL Download Main module](./SQLDownloadMain.py) scripts become the final tables required for our data model.

# Loading to BigQuery
The final step of the ETL process involves loading the structured data into a database. In our case, we chose BigQuery as our cloud data repository. The loading [Load Files to BQ](./LoadFilesBQ.py) scripts format data types appropriately and upload the processed data into BigQuery.

# Automation with Virtual Machines
To achieve a fully automated pipeline, it's important to have a tool capable of executing scripts automatically at specified intervals. In our case, we've chosen to deploy a virtual machine (VM) on which all our scripts are hosted, configuring cron jobs to run them at specific times.


These scripts don't involve any AI models; however, they handle robust data processing tasks, including files containing up to 10 million rows. Despite their robustness, even a low-end VM can execute these scripts without issues. The primary challenge here is ensuring sufficient RAM for correctly performing data preprocessing tasks, for which at least 16 GB of RAM is recommended.

This project doesn't cover the specific configurations required to execute scripts on a VM. Nevertheless, there are numerous tutorials available online that can guide you through this process.

# Power BI Dashboard

With the data now stored in BigQuery, visualization tools like Power BI can directly query the repository and retrieve updated datasets. Within Power BI:
- A data model is created to relate the different tables.
- Visualizations and measures are built to produce clear insights into KPI performance.
- UX and UI are emphasized, with intuitive layouts and interactive elements.

It's important to mention that the use of a cloud repository significantly resolves challenges related to automating the entire process. To enable automated updates in Power BI, we needed to avoid limitations associated with directly querying traditional databases (such as restrictions on simultaneous sessions), querying local CSV files, managing access keys, and handling slow download speeds. BigQuery effectively addresses many of these challenges.
